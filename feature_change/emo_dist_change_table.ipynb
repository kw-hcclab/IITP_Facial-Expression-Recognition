{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NanumGothicLight']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumGothicLight') # For Windows\n",
    "print(plt.rcParams['font.family'])\n",
    "import json  \n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(data, path, face):    \n",
    "    img_array = np.fromfile(path, np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    # img = cv2.imread(path, cv2.IMREAD_COLOR)    \n",
    "    drawing_image = img.copy()\n",
    "    plt.imshow(cv2.cvtColor(drawing_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.savefig('../data/image_analysis/imagesave/'+face+'facepic.jpg', facecolor='#ffffff', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_flow(emo, order, face):\n",
    "    plt.rc('ytick', labelsize=17)  # y축 눈금 폰트 크기\n",
    "    plt.rc('xtick', labelsize=17)  # x축 눈금 폰트 크기 \n",
    "    plt.rc('legend', fontsize=17)  # 범례 폰트 크기\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax = plt.subplots(len(emotion[emo]), 1, figsize = (20,20))\n",
    "    # plt.suptitle(face)\n",
    "    for idx in range(len(emotion[emo])):\n",
    "        target_list = [i['from'].replace('-',' - ') + \" -> \" + i['dest'].replace('-', ' - ') for i in emotion[emo][idx]['landmark_pair']]\n",
    "        temp_plot = sns.lineplot(data = df[target_list], markers=False, dashes=False, ax=ax[idx], palette=\"tab10\")        \n",
    "        ax[idx].axvline(order, 0, 1, color='red', linestyle='--', linewidth=2)\n",
    "        temp_plot.set_title(emotion[emo][idx]['name'], fontsize=30)\n",
    "    plt.savefig('../data/image_analysis/imagesave/'+face+'.jpg', facecolor='#ffffff', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jongm\\Desktop\\MINI\\projects\\Facial-Expression-Recognition\\feature_change\\emo_dist_change_table.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jongm/Desktop/MINI/projects/Facial-Expression-Recognition/feature_change/emo_dist_change_table.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image_order \u001b[39m=\u001b[39m \u001b[39m271\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jongm/Desktop/MINI/projects/Facial-Expression-Recognition/feature_change/emo_dist_change_table.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../data/image_analysis/emotion_rule_weight.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jongm/Desktop/MINI/projects/Facial-Expression-Recognition/feature_change/emo_dist_change_table.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         emotion \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jongm/Desktop/MINI/projects/Facial-Expression-Recognition/feature_change/emo_dist_change_table.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m fa \u001b[39min\u001b[39;00m people_list:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jongm/Desktop/MINI/projects/Facial-Expression-Recognition/feature_change/emo_dist_change_table.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     face \u001b[39m=\u001b[39m fa \u001b[39m+\u001b[39m emo_kr\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Happy, Sadness, Surprise, Anger\n",
    "emo = {'웃기':'Happy',\n",
    "        '슬픔': 'Sadness',\n",
    "        '놀람': 'Surprise',\n",
    "        '화남': 'Anger'}\n",
    "people_list = ['소정뉴']\n",
    "emo_kr = '화남'\n",
    "image_order = 271\n",
    "with open('../data/image_analysis/emotion_rule_weight.json', 'r') as f:\n",
    "        emotion = json.load(f)\n",
    "\n",
    "for fa in people_list:\n",
    "    face = fa + emo_kr\n",
    "    df = pd.read_csv('../data/image_analysis/'+face+'/'+face+'.csv')\n",
    "    show_flow(emo[emo_kr], image_order, face)\n",
    "    show_img(df, '../data/image_analysis/'+face+'/' + str(image_order) + '.jpg', face)\n",
    "#     print(pd.DataFrame(df.iloc[image_order]).to_dict()[image_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.iloc[180:280].mean().round(2)).iloc[:-1][0].to_clipboard(index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/image_analysis/emotion_rule.json', 'r') as f:\n",
    "#     json_data = json.load(f)\n",
    "# rule = json_data\n",
    "# df = pd.read_clipboard()\n",
    "# df['alpha'] = df['alpha'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = []\n",
    "# #감정표정 종류\n",
    "# start = 0\n",
    "# for i in json_data:\n",
    "#     #표정 종류\n",
    "#     idx = 0\n",
    "#     for j in json_data[i]:\n",
    "#         # #pair\n",
    "#         # for la in j['landmark_pair']:\n",
    "#         end = len(j['landmark_pair'])\n",
    "#         # print(df[start:start+end])\n",
    "#         # print(start, start+end)\n",
    "#         rule[i][idx]['landmark_pair'] = df[start:start+end].to_dict('records')\n",
    "#         idx += 1\n",
    "#         start = start+end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/image_analysis/emotion_rule_weight.json', 'w', encoding='utf-8') as make_file:\n",
    "#     json.dump(rule, make_file, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = []\n",
    "# for i in emotion:\n",
    "#     for j in emotion[i]:\n",
    "#         li = li + j['landmark_pair']\n",
    "# pd.DataFrame(li).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tfface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e318f884811f55877d06bbc5456a4670ca1397bb6e20c4ae81dc4f96f0847c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
